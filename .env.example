# Local Deep Researcher (DToR Version) Configuration
# Copy this file to .env and adjust the values as needed

# -- LLM Configuration --

# LLM Provider: "ollama" or "lmstudio"
LLM_PROVIDER=ollama

# Local LLM model name (default if not specified: gpt-oss:20b)
LOCAL_LLM=gpt-oss:20b

# Ollama and LM Studio base URL
OLLAMA_BASE_URL=http://localhost:11434/
LMSTUDIO_BASE_URL=http://localhost:1234/v1

# Timeout (seconds) for local Ollama/LMStudio calls before falling back to OpenAI
LOCAL_LLM_TIMEOUT_SECONDS=600

# OpenAI Configuration (enables cloud fallback and required when LLM_PROVIDER=openai)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-5-nano
# Comma-separated list of backup OpenAI models (will be tried in order)
OPENAI_FALLBACK_MODELS=gpt-5-nano,gpt-5-mini,gpt-5

# Whether to strip <think> tokens from model responses
STRIP_THINKING_TOKENS=True

# -- Research Mode Configuration --

# Research mode: "single" for standard research, "dtor" for Deep Tree of Research
RESEARCH_MODE=single

# -- Web Search Configuration --

# Search API: "perplexity", "tavily", "duckduckgo", or "searxng"
SEARCH_API=duckduckgo

# API key for Tavily (only needed if SEARCH_API=tavily)
TAVILY_API_KEY=

# API key for Perplexity (only needed if SEARCH_API=perplexity)
PERPLEXITY_API_KEY=

# SearXNG instance URL (only needed if SEARCH_API=searxng)
SEARXNG_URL=http://localhost:8888

# Maximum number of web research loops (for single mode)
MAX_WEB_RESEARCH_LOOPS=3

# Whether to fetch full page content (for duckduckgo)
FETCH_FULL_PAGE=True

# -- Local RAG Configuration --

# Whether to use local vector store for RAG (currently checked *before* web search)
USE_LOCAL_RAG=False

# Path(s) to local vector store directory/directories (comma-separated if multiple)
# Only required when USE_LOCAL_RAG=True
# Example: VECTOR_STORE_PATHS=/path/to/store1,/path/to/store2
# VECTOR_STORE_PATHS=/path/to/your/vector_store/

# Number of chunks to retrieve from local vector store
LOCAL_RESULTS_COUNT=5

# Embedding model to use for vector store queries
EMBEDDING_MODEL=BAAI/bge-m3

# -- Tree of Thoughts (DToR) Configuration (only used if RESEARCH_MODE=dtor) --

# Maximum number of branches to explore
MAX_BRANCHES=3

# Maximum depth of each branch
MAX_BRANCH_DEPTH=2
# Budget of research nodes per branch
NODES_PER_BRANCH=100 
