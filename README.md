# Local Deep Researcher ‚öôÔ∏èüìö

Local, resilient research assistant built on LangGraph. Install [Ollama](https://ollama.com/download) (or bring your own OpenAI-compatible host such as LMStudio), then the agent fans out to web search (DuckDuckGo, SearXNG, Tavily, Perplexity), optionally pulls from local vector stores, and keeps iterating until the topic is covered. Choose between a focused single-path loop or a Deep Tree of Research (DToR) exploration that manages multiple perspectives in parallel, culminating in a consolidated deep research report.

## What you get ‚ú®
- **LLM flexibility:** Ollama or LMStudio by default, with automatic fallback to OpenAI if permitted.
- **Two research modes:** linear loop for quick digs, DToR (Deep Tree of Research) for broad, multi-branch studies.
- **Search & RAG plug-ins:** swap web backends, query FET sensor data/code/papers stored in Chroma, or fall back to legacy stores.
- **LangGraph Studio integration:** inspect state, resume runs, and debug with structured logs.
- **Headless & production workflows:** CLI with SQLite checkpoints plus cluster-ready orchestration scripts.
- **Artifact trail:** per-iteration logs, branch syntheses, final reports, and optional FET fabrication parameters.

Documentation lives under [`docs/`](docs). Start with:
- [`docs/SYSTEM_OVERVIEW.md`](docs/SYSTEM_OVERVIEW.md) ‚Äì architecture map, modules, state objects, failover behavior.
- [`docs/RESEARCH_WORKFLOWS.md`](docs/RESEARCH_WORKFLOWS.md) ‚Äì single-path loop, DToR routing, batching scripts.
- [`docs/MODALITY_RAG.md`](docs/MODALITY_RAG.md) ‚Äì configuration and query tips for FET/code/paper retrieval.

## Workstation workflow (LangGraph Studio) üíª
1. **Configuration:** copy `.env.example` ‚Üí `.env` only if you need repeatable server settings. On laptops you can leave it out and use the Studio UI‚Äîremember that environment variables always override UI values.
2. **Raise the file limit:** heavy RAG, especially FET stores, can exhaust descriptors. On macOS/Linux run `ulimit -n 1048575` before launching.
3. **Start Studio:**
   ```bash
   uvx --refresh --from "langgraph-cli[inmem]==0.1.89" --with-editable . --python 3.11 langgraph dev
   ```
4. **Sign in first:** click the LangSmith icon in the top-left as soon as the UI opens. Submitting a run while logged out triggers a redirect that looks like a stalled workflow.
5. **Pick your graph:** `ollama_deep_researcher` for the single loop, `deep_researcher_dtor` for Deep Tree of Research.
6. **Configure & run:** adjust settings in Studio (or let `.env` drive them), enter a topic, and launch.

> üïπÔ∏è Running DToR unattended? Keep the terminal closed and start `python auto_continue.py`. It watches the logs and auto-clicks ‚ÄúContinue‚Äù whenever Studio pauses for confirmation.

## Batch & production options üöÄ
- **Headless CLI:**
  ```bash
  python run_batch.py "llama quantization techniques" \
    -c research_mode dtor -c max_branches 3 -c max_branch_depth 2
  ```
  Outputs stream to stdout and are saved in `synthesis_branches_and_final/` plus `logs/`.
- **Cluster orchestration:**
  - `dtor_orchestration/run_batch_cluster.py` runs DToR with SQLite checkpoints, wall-time awareness, and signal hooks for SLURM/PBS.
  - `dtor_orchestration/run_diversify_only.py` (Stage 1) precomputes perspectives so later jobs can focus on branch execution.
  Both respect the same `.env` used locally‚Äîprototype in Studio, ship to the cluster with the same settings.

## Configuration cheat sheet üß≠
Priority: **environment** ‚Üí **Studio overrides** ‚Üí **code defaults** (`Configuration` class).

Key toggles:
- `LLM_PROVIDER`, `LOCAL_LLM`, `OLLAMA_BASE_URL`, `LMSTUDIO_BASE_URL`
- `SEARCH_API` + API keys (`TAVILY_API_KEY`, `PERPLEXITY_API_KEY`, `SEARXNG_URL`)
- `RESEARCH_MODE`, `MAX_WEB_RESEARCH_LOOPS`, `MAX_BRANCHES`, `MAX_BRANCH_DEPTH`
- RAG flags: `USE_LOCAL_RAG`, `ENABLE_FET_RAW_DATA`, `ENABLE_CODE_RETRIEVAL`, `ENABLE_PAPER_RETRIEVAL`
- FET settings: vector store paths, top-k values, embedding model (`BAAI/bge-m3` by default)

## Outputs & logs üóÉÔ∏è
- `logs/` ‚Äì analysis, query generation, routing, reflection, finalisation logs per run.
- `synthesis_branches_and_final/<topic>/` ‚Äì DToR branch syntheses and final merged reports.
- Studio UI ‚Äì interactive state tree and checkpoints.

## Troubleshooting highlights üõ†Ô∏è
- Studio won‚Äôt open? copy the printed URL (usually `http://127.0.0.1:2024`).
- Forced login mid-run? sign in via the LangSmith icon before submitting.
- Slow or timing out? disable `FETCH_FULL_PAGE`, drop `MAX_WEB_RESEARCH_LOOPS`, or switch to a faster model.
- JSON parsing hiccups? the agent already strips code fences, but switching models or tightening prompts can help.
- LMStudio/Ollama errors? verify the local server is running and the model is pulled.

## Repo layout üóÇÔ∏è
```
src/ollama_deep_researcher/
  configuration.py      # Pydantic config schema & env merge logic
  graph.py              # Single-path research loop
  dtor_graph.py          # Deep Tree of Research wrapper graph
  dtor_nodes.py          # DToR node logic (diversify, analyse, route, synthesise)
  state.py / dtor_state.py  # Dataclasses for graph state
  utils.py              # Search adapters, RAG helpers, formatting utilities
  prompts.py            # Prompt templates (single + FET-specific)
  lmstudio.py, openai_wrapper.py  # Model wrappers
run_batch.py            # Headless CLI entrypoint
dtor_orchestration/      # Cluster orchestration scripts
auto_continue.py        # Studio auto-click helper for long DToR runs
docs/                   # Deep-dive documentation
```

## License
MIT ‚Äî see [`LICENSE`](LICENSE).

## Thanks
- Built with LangGraph/LangChain and a collection of community search libraries.
- Contributions that improve stability, docs, or new modalities are welcome!
